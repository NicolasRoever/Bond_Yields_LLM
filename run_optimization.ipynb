{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Code\n",
    "\n",
    "In this sandbox, I optimize the prompts for the LLM chain using Dspy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "This import all the packages, separate functions and sets the LLM to the chain defined in the main module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import groq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import dspy\n",
    "import numpy as np\n",
    "from dspy.evaluate.metrics import answer_exact_match\n",
    "import pandas as pd\n",
    "from IPython.core.display import Markdown\n",
    "from dspy import Example\n",
    "from dspy.teleprompt import BootstrapFewShot\n",
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n",
    "from dspy.evaluate import Evaluate\n",
    "from module_v002 import FullLLMChain\n",
    "from optimize import passage_similarity_metric, custom_evaluation_function, similar_score_metric, evaluate_expectations_metric\n",
    "from data.preprocess import create_dspy_examples_train_test_validation_sets\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "llama3_8b = dspy.OllamaLocal(model = \"llama3:8b\",\n",
    "                             temperature = 0,\n",
    "                             max_tokens = 800)\n",
    "\n",
    "gpt35turbo = dspy.OpenAI(model = \"gpt-3.5-turbo-0125\",\n",
    "                         api_key = openai_api_key,\n",
    "                         temperature = 0,\n",
    "                         max_tokens = 800,\n",
    "                         model_type = \"chat\")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"data/300_snippets_transcripts_all_labeled_v002.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set, validation_set = create_dspy_examples_train_test_validation_sets(\n",
    "    data=data, \n",
    "    train_size=50, \n",
    "    test_size=50,\n",
    "    validation_size=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "This optimizes the prompts and saves the optimized LLM as well as the last 10 instances prompted to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dspy.settings.configure(lm=gpt35turbo)\n",
    "full_llm_chain = FullLLMChain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to sample between 1 and 2 traces per predictor.\n",
      "Will attempt to train 4 candidate sets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 22.0 / 50  (44.0): 100%|██████████| 50/50 [00:24<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 22.0 / 50  (44.0%)\n",
      "Score: 44.0 for set: [0, 0, 0]\n",
      "New best score: 44.0 for seed -3\n",
      "Scores so far: [44.0]\n",
      "Best score: 44.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 21.0 / 47  (44.7):  94%|█████████▍| 47/50 [00:24<00:02,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000861588493462}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 23.0 / 50  (46.0): 100%|██████████| 50/50 [00:25<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 23.0 / 50  (46.0%)\n",
      "Score: 46.0 for set: [1, 1, 1]\n",
      "New best score: 46.0 for seed -2\n",
      "Scores so far: [44.0, 46.0]\n",
      "Best score: 46.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [00:12<01:55,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 6 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.5 / 5  (30.0):  10%|█         | 5/50 [00:02<00:21,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000830910832171}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.5 / 7  (35.7):  12%|█▏        | 6/50 [00:04<00:33,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.0 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000046818144722}\n",
      "Backing off 0.0 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000443977300059}\n",
      "Backing off 0.3 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000918079761941}\n",
      "Backing off 1.9 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000830910832171}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.5 / 8  (43.8):  16%|█▌        | 8/50 [00:05<00:31,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000573580416064}\n",
      "Backing off 1.9 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000443977300059}\n",
      "Backing off 0.8 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000918079761941}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.5 / 9  (38.9):  18%|█▊        | 9/50 [00:08<00:52,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000046818144722}\n",
      "Backing off 0.3 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000573580416064}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.5 / 10  (45.0):  20%|██        | 10/50 [00:10<00:58,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.1 seconds after 3 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000918079761941}\n",
      "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.700075073737791}\n",
      "Backing off 0.1 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000046818144722}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 5.0 / 12  (41.7):  24%|██▍       | 12/50 [00:12<00:48,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.0 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000731129226365}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 6.0 / 13  (46.2):  26%|██▌       | 13/50 [00:13<00:44,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.1 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000731129226365}\n",
      "Backing off 4.8 seconds after 4 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000918079761941}\n",
      "Backing off 1.0 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000002154634184}\n",
      "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000867486203216}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 6.0 / 14  (42.9):  28%|██▊       | 14/50 [00:16<01:02,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 3.4 seconds after 3 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000731129226365}\n",
      "Backing off 1.5 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000867486203216}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 6.0 / 15  (40.0):  30%|███       | 15/50 [00:18<01:01,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.2 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000002154634184}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 6.5 / 16  (40.6):  32%|███▏      | 16/50 [00:19<00:50,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.3 seconds after 3 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000867486203216}\n",
      "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000218666568226}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 6.5 / 17  (38.2):  34%|███▍      | 17/50 [00:21<00:53,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 7.0 seconds after 4 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000731129226365}\n",
      "Backing off 0.9 seconds after 3 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000002154634184}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.5 / 19  (44.7):  38%|███▊      | 19/50 [00:23<00:42,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.6 seconds after 4 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000867486203216}\n",
      "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000508214286507}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.5 / 20  (47.5):  40%|████      | 20/50 [00:24<00:39,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.8 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000147956470134}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 10.5 / 21  (50.0):  42%|████▏     | 21/50 [00:26<00:40,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 5.8 seconds after 5 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000867486203216}\n",
      "Backing off 0.1 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000508214286507}\n",
      "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000002154634184}\n",
      "Backing off 1.0 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000147956470134}\n",
      "Backing off 1.6 seconds after 3 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000508214286507}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 11.5 / 23  (50.0):  46%|████▌     | 23/50 [00:30<00:43,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 5.8 seconds after 5 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000731129226365}\n",
      "Backing off 0.5 seconds after 3 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000147956470134}\n",
      "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000161182678746}\n",
      "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000781303796165}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.5 / 24  (52.1):  48%|████▊     | 24/50 [00:35<01:06,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000508214286507}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.0 / 25  (52.0):  50%|█████     | 25/50 [00:36<00:54,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.5 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000161182678746}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.0 / 26  (50.0):  52%|█████▏    | 26/50 [00:37<00:46,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.3 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000508214286507}\n",
      "Backing off 4.1 seconds after 6 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000731129226365}\n",
      "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000867486203216}\n",
      "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000058123062843}\n",
      "Backing off 3.3 seconds after 3 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000161182678746}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.0 / 27  (51.9):  54%|█████▍    | 27/50 [00:39<00:43,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.0 seconds after 3 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000508214286507}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.0 / 28  (53.6):  56%|█████▌    | 28/50 [00:41<00:38,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000951439797125}\n",
      "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000372896251305}\n",
      "Backing off 3.1 seconds after 4 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000508214286507}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.0 / 29  (51.7):  58%|█████▊    | 29/50 [00:43<00:38,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 5.2 seconds after 4 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000161182678746}\n",
      "Backing off 37.8 seconds after 7 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000731129226365}\n",
      "Backing off 1.9 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000372896251305}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.5 / 30  (51.7):  60%|██████    | 30/50 [00:45<00:37,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000653312585743}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.5 / 31  (50.0):  62%|██████▏   | 31/50 [00:48<00:43,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.8 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000653312585743}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.5 / 32  (48.4):  64%|██████▍   | 32/50 [00:49<00:35,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000372896251305}\n",
      "Backing off 0.8 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000628863505403}\n",
      "Backing off 4.2 seconds after 5 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000161182678746}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 17.0 / 34  (50.0):  68%|██████▊   | 34/50 [00:51<00:24,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.9 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000628863505403}\n",
      "Backing off 1.3 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000372896251305}\n",
      "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000871636345122}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.0 / 36  (50.0):  72%|███████▏  | 36/50 [00:55<00:21,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.0 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000747210817275}\n",
      "Backing off 4.5 seconds after 6 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000161182678746}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.0 / 37  (48.6):  74%|███████▍  | 37/50 [00:57<00:22,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 3.9 seconds after 3 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000628863505403}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.0 / 38  (47.4):  76%|███████▌  | 38/50 [00:58<00:17,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.9 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000747210817275}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.0 / 40  (47.5):  80%|████████  | 40/50 [01:00<00:13,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000622048499311}\n",
      "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000429107718127}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.0 / 42  (45.2):  84%|████████▍ | 42/50 [01:03<00:10,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.8 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000959745507211}\n",
      "Backing off 1.7 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000429107718127}\n",
      "Backing off 7.8 seconds after 4 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000628863505403}\n",
      "Backing off 62.0 seconds after 7 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000161182678746}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 20.5 / 44  (46.6):  88%|████████▊ | 44/50 [01:06<00:08,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.5 seconds after 3 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000429107718127}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 26.5 / 50  (53.0): 100%|██████████| 50/50 [02:07<00:00,  2.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 26.5 / 50  (53.0%)\n",
      "Score: 53.0 for set: [2, 2, 1]\n",
      "New best score: 53.0 for seed -1\n",
      "Scores so far: [44.0, 46.0, 53.0]\n",
      "Best score: 53.0\n",
      "Average of max per entry across top 1 scores: 0.53\n",
      "Average of max per entry across top 2 scores: 0.69\n",
      "Average of max per entry across top 3 scores: 0.74\n",
      "Average of max per entry across top 5 scores: 0.74\n",
      "Average of max per entry across top 8 scores: 0.74\n",
      "Average of max per entry across top 9999 scores: 0.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [00:10<02:46,  3.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 4 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.0 / 29  (41.4):  58%|█████▊    | 29/50 [00:16<00:08,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000841651670915}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.0 / 34  (44.1):  68%|██████▊   | 34/50 [00:21<00:12,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000689378198371}\n",
      "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000767384656629}\n",
      "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000367853147456}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.0 / 35  (45.7):  70%|███████   | 35/50 [00:22<00:15,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.700035822123253}\n",
      "Backing off 1.4 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000689378198371}\n",
      "Backing off 0.3 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000899410502422}\n",
      "Backing off 2.0 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000367853147456}\n",
      "Backing off 0.2 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000767384656629}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.0 / 36  (44.4):  72%|███████▏  | 36/50 [00:26<00:24,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 2.2 seconds after 3 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000689378198371}\n",
      "Backing off 1.6 seconds after 3 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000767384656629}\n",
      "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000040916634462}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 17.0 / 37  (45.9):  74%|███████▍  | 37/50 [00:29<00:29,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 2.7 seconds after 3 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000367853147456}\n",
      "Backing off 0.8 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000040916634462}\n",
      "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000551245177778}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.0 / 38  (47.4):  76%|███████▌  | 38/50 [00:31<00:23,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 6.3 seconds after 4 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000767384656629}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.5 / 40  (46.2):  80%|████████  | 40/50 [00:33<00:15,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.5 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000551245177778}\n",
      "Backing off 1.4 seconds after 3 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000040916634462}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.5 / 41  (45.1):  82%|████████▏ | 41/50 [00:34<00:13,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 7.8 seconds after 4 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000367853147456}\n",
      "Backing off 0.2 seconds after 3 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000551245177778}\n",
      "Backing off 4.9 seconds after 4 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000040916634462}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.5 / 42  (44.0):  84%|████████▍ | 42/50 [00:39<00:18,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000759572844936}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.0 / 43  (44.2):  86%|████████▌ | 43/50 [00:41<00:15,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000217087495961}\n",
      "Backing off 0.5 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000759572844936}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 20.5 / 45  (45.6):  90%|█████████ | 45/50 [00:44<00:08,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.0 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000217087495961}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 21.5 / 46  (46.7):  92%|█████████▏| 46/50 [00:45<00:06,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 10.0 seconds after 5 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000040916634462}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 21.5 / 50  (43.0): 100%|██████████| 50/50 [00:57<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 21.5 / 50  (43.0%)\n",
      "Score: 43.0 for set: [2, 2, 2]\n",
      "Scores so far: [44.0, 46.0, 53.0, 43.0]\n",
      "Best score: 53.0\n",
      "Average of max per entry across top 1 scores: 0.53\n",
      "Average of max per entry across top 2 scores: 0.69\n",
      "Average of max per entry across top 3 scores: 0.74\n",
      "Average of max per entry across top 5 scores: 0.86\n",
      "Average of max per entry across top 8 scores: 0.86\n",
      "Average of max per entry across top 9999 scores: 0.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [00:06<02:44,  3.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 3 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.0 / 5  (60.0):  10%|█         | 5/50 [00:03<00:23,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.3 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000764488006934}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.0 / 6  (66.7):  12%|█▏        | 6/50 [00:04<00:29,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000655695756235}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.0 / 7  (57.1):  14%|█▍        | 7/50 [00:06<00:45,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000920331075348}\n",
      "Backing off 0.5 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000764488006934}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.0 / 8  (50.0):  16%|█▌        | 8/50 [00:07<00:45,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000685412389009}\n",
      "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000029014551009}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.5 / 9  (50.0):  18%|█▊        | 9/50 [00:10<01:05,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.7 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000029014551009}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.5 / 10  (45.0):  20%|██        | 10/50 [00:12<01:03,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.0 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000685412389009}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 5.5 / 12  (45.8):  22%|██▏       | 11/50 [00:13<00:59,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.1 seconds after 3 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000029014551009}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 5.5 / 13  (42.3):  26%|██▌       | 13/50 [00:15<00:47,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000630863897989}\n",
      "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000824136570934}\n",
      "Backing off 2.0 seconds after 3 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000685412389009}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 6.5 / 14  (46.4):  28%|██▊       | 14/50 [00:18<01:01,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.0 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000505412363753}\n",
      "Backing off 1.0 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000008055591119}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 6.5 / 15  (43.3):  30%|███       | 15/50 [00:20<01:02,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000029014551009}\n",
      "Backing off 3.7 seconds after 4 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000685412389009}\n",
      "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000505412363753}\n",
      "Backing off 1.8 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000008055591119}\n",
      "Backing off 0.0 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000505412363753}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.5 / 16  (46.9):  32%|███▏      | 16/50 [00:24<01:19,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.1 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000029014551009}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.5 / 17  (44.1):  34%|███▍      | 17/50 [00:25<01:05,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000585918821479}\n",
      "Backing off 1.9 seconds after 3 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000008055591119}\n",
      "Backing off 0.0 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000250295546432}\n",
      "Backing off 2.3 seconds after 3 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000029014551009}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.0 / 19  (42.1):  38%|███▊      | 19/50 [00:29<00:56,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.2 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000585918821479}\n",
      "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000744451195007}\n",
      "Backing off 5.8 seconds after 4 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000008055591119}\n",
      "Backing off 3.3 seconds after 3 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000585918821479}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.0 / 20  (40.0):  40%|████      | 20/50 [00:32<01:03,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 2.0 seconds after 4 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000029014551009}\n",
      "Backing off 0.3 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000744451195007}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.0 / 22  (40.9):  44%|████▍     | 22/50 [00:36<01:00,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.700003278867638}\n",
      "Backing off 0.5 seconds after 4 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000585918821479}\n",
      "Backing off 15.3 seconds after 5 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000029014551009}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.0 / 23  (39.1):  46%|████▌     | 23/50 [00:38<00:52,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 4.3 seconds after 5 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000008055591119}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 10.0 / 24  (41.7):  48%|████▊     | 24/50 [00:39<00:46,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 13.9 seconds after 5 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000585918821479}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 10.5 / 26  (40.4):  52%|█████▏    | 26/50 [00:42<00:37,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000208853690072}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.0 / 29  (41.4):  58%|█████▊    | 29/50 [00:45<00:28,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.8 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000904891826528}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.0 / 30  (43.3):  60%|██████    | 30/50 [00:47<00:26,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000863606094162}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.0 / 31  (41.9):  62%|██████▏   | 31/50 [00:51<00:43,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.8 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.700049223788767}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.0 / 33  (42.4):  64%|██████▍   | 32/50 [00:53<00:36,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.7 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000863606094162}\n",
      "Backing off 15.3 seconds after 6 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000029014551009}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.0 / 34  (44.1):  68%|██████▊   | 34/50 [00:54<00:23,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.1 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.700049223788767}\n",
      "Backing off 24.7 seconds after 6 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000585918821479}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.0 / 35  (45.7):  70%|███████   | 35/50 [00:56<00:24,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 2.4 seconds after 3 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.700049223788767}\n",
      "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000294652014744}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 17.0 / 36  (47.2):  72%|███████▏  | 36/50 [00:57<00:18,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.0 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000294652014744}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.0 / 38  (47.4):  76%|███████▌  | 38/50 [01:02<00:20,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 2.8 seconds after 3 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000294652014744}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.0 / 40  (45.0):  80%|████████  | 40/50 [01:05<00:15,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.8 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.700074857218905}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.0 / 41  (43.9):  82%|████████▏ | 41/50 [01:07<00:16,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 7.7 seconds after 4 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000294652014744}\n",
      "Backing off 0.3 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000106635928416}\n",
      "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000364343397876}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.0 / 42  (42.9):  84%|████████▍ | 42/50 [01:09<00:15,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.9 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000364343397876}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.0 / 43  (41.9):  86%|████████▌ | 43/50 [01:11<00:13,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 62.3 seconds after 7 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000029014551009}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.5 / 44  (42.0):  88%|████████▊ | 44/50 [01:14<00:12,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 3.2 seconds after 3 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000364343397876}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 20.0 / 50  (40.0): 100%|██████████| 50/50 [02:16<00:00,  2.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 20.0 / 50  (40.0%)\n",
      "Score: 40.0 for set: [1, 1, 1]\n",
      "Scores so far: [44.0, 46.0, 53.0, 43.0, 40.0]\n",
      "Best score: 53.0\n",
      "Average of max per entry across top 1 scores: 0.53\n",
      "Average of max per entry across top 2 scores: 0.69\n",
      "Average of max per entry across top 3 scores: 0.74\n",
      "Average of max per entry across top 5 scores: 0.89\n",
      "Average of max per entry across top 8 scores: 0.89\n",
      "Average of max per entry across top 9999 scores: 0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:03<02:35,  3.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 24.0 / 50  (48.0): 100%|██████████| 50/50 [00:22<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 24.0 / 50  (48.0%)\n",
      "Score: 48.0 for set: [1, 1, 1]\n",
      "Scores so far: [44.0, 46.0, 53.0, 43.0, 40.0, 48.0]\n",
      "Best score: 53.0\n",
      "Average of max per entry across top 1 scores: 0.53\n",
      "Average of max per entry across top 2 scores: 0.76\n",
      "Average of max per entry across top 3 scores: 0.81\n",
      "Average of max per entry across top 5 scores: 0.92\n",
      "Average of max per entry across top 8 scores: 0.94\n",
      "Average of max per entry across top 9999 scores: 0.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [00:06<02:25,  3.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 3 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2 / 4  (50.0):   8%|▊         | 4/50 [00:01<00:16,  2.77it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000287617851588}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3 / 5  (60.0):  10%|█         | 5/50 [00:02<00:26,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000559161067597}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 4 / 6  (66.7):  12%|█▏        | 6/50 [00:03<00:20,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000964760855238}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 6 / 9  (66.7):  18%|█▊        | 9/50 [00:05<00:23,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000398559506662}\n",
      "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000499059254902}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 6 / 10  (60.0):  20%|██        | 10/50 [00:06<00:36,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.8 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000205846660467}\n",
      "Backing off 0.2 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000499059254902}\n",
      "Backing off 0.3 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000398559506662}\n",
      "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000964760855238}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 7 / 11  (63.6):  22%|██▏       | 11/50 [00:09<00:55,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000887730381801}\n",
      "Backing off 0.5 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000205846660467}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 7 / 12  (58.3):  24%|██▍       | 12/50 [00:10<00:54,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000964760855238}\n",
      "Backing off 1.7 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000887730381801}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 9 / 14  (64.3):  28%|██▊       | 14/50 [00:12<00:41,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 2.0 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000964760855238}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 10 / 15  (66.7):  30%|███       | 15/50 [00:13<00:39,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.0 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000956374000822}\n",
      "Backing off 1.4 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000956374000822}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 11 / 17  (64.7):  32%|███▏      | 16/50 [00:15<00:46,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.3 seconds after 3 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000887730381801}\n",
      "Backing off 1.9 seconds after 3 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000964760855238}\n",
      "Backing off 5.7 seconds after 4 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000887730381801}\n",
      "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000849083725643}\n",
      "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000344745963888}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 12 / 19  (63.2):  38%|███▊      | 19/50 [00:18<00:35,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.1 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000344745963888}\n",
      "Backing off 0.3 seconds after 4 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000964760855238}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 13 / 20  (65.0):  40%|████      | 20/50 [00:21<00:44,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000656664557154}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 15 / 22  (68.2):  44%|████▍     | 22/50 [00:22<00:32,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.8 seconds after 5 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000964760855238}\n",
      "Backing off 1.0 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000048211210192}\n",
      "Backing off 1.6 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000656664557154}\n",
      "Backing off 0.3 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000344745963888}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 16 / 24  (66.7):  48%|████▊     | 24/50 [00:26<00:39,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 30.1 seconds after 6 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000964760855238}\n",
      "Backing off 2.6 seconds after 3 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000656664557154}\n",
      "Backing off 0.3 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000344745963888}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 16 / 25  (64.0):  50%|█████     | 25/50 [00:28<00:37,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000048211210192}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 17 / 26  (65.4):  52%|█████▏    | 26/50 [00:29<00:31,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 2.4 seconds after 3 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000344745963888}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 18 / 27  (66.7):  54%|█████▍    | 27/50 [00:30<00:28,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.2 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000048211210192}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 19 / 28  (67.9):  56%|█████▌    | 28/50 [00:31<00:29,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.6 seconds after 4 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000656664557154}\n",
      "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000083328992484}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.5 / 29  (67.2):  58%|█████▊    | 29/50 [00:34<00:35,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.8 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000083328992484}\n",
      "Backing off 0.2 seconds after 4 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000344745963888}\n",
      "Backing off 13.8 seconds after 5 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000656664557154}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.5 / 31  (62.9):  62%|██████▏   | 31/50 [00:37<00:30,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 14.8 seconds after 5 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000344745963888}\n",
      "Backing off 3.0 seconds after 3 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000083328992484}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 23.5 / 36  (65.3):  72%|███████▏  | 36/50 [00:42<00:15,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.9 seconds after 4 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000083328992484}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 25.5 / 38  (67.1):  76%|███████▌  | 38/50 [00:45<00:13,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000124110942705}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 26.5 / 40  (66.2):  80%|████████  | 40/50 [00:47<00:10,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.4 seconds after 2 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000124110942705}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 27.5 / 43  (64.0):  86%|████████▌ | 43/50 [00:51<00:08,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 28.4 seconds after 6 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000656664557154}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 27.5 / 44  (62.5):  88%|████████▊ | 44/50 [00:52<00:06,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 2.8 seconds after 3 tries calling function <function GPT3.request at 0x10a33ed40> with kwargs {'temperature': 0.7000124110942705}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 30.0 / 50  (60.0): 100%|██████████| 50/50 [01:22<00:00,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 30.0 / 50  (60.0%)\n",
      "Score: 60.0 for set: [1, 1, 1]\n",
      "New best score: 60.0 for seed 3\n",
      "Scores so far: [44.0, 46.0, 53.0, 43.0, 40.0, 48.0, 60.0]\n",
      "Best score: 60.0\n",
      "Average of max per entry across top 1 scores: 0.6\n",
      "Average of max per entry across top 2 scores: 0.78\n",
      "Average of max per entry across top 3 scores: 0.9\n",
      "Average of max per entry across top 5 scores: 0.93\n",
      "Average of max per entry across top 8 scores: 0.96\n",
      "Average of max per entry across top 9999 scores: 0.96\n",
      "7 candidate programs found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "config = dict(max_bootstrapped_demos=2, max_labeled_demos=1, max_rounds=1, max_errors=1, num_candidate_programs = 4)\n",
    "teleprompter = BootstrapFewShotWithRandomSearch(metric=evaluate_expectations_metric, **config)\n",
    "optimized_llm = teleprompter.compile(full_llm_chain, trainset=train_set, valset=test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_llm.save(\"optimized_llm_chains/optimized_gpt_chain_v010.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama3_8b.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "---CONTEXT---\n",
      "    You are an experienced financial analyst known for your expertise in evaluating and interpreting expectations related to the financial stability and solvency of countries.\n",
      "\n",
      "    ---TASK---\n",
      "    Please assess the expectation towards the solvency of the country mentioned in the given text excerpt. This includes evaluating the country's financial stability and ability to meet its obligations.    \n",
      "    \n",
      "    ---GUIDELINES---\n",
      "    - Use the following scale for your assessment: -2 = very negative, -1 = somewhat negative, 0 = neutral, 1 = somewhat positive, 2 = very positive\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Country Keyword: keyword that represents a country\n",
      "\n",
      "Country Role: role of the country in the excerpt of a financial services company's earnings call transcript\n",
      "\n",
      "Excerpt: excerpt from a financial services company's earnings conference call\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: one of: [-2, -1, 0, 1, 2]. Only respond wiht a single digit and nothing else. Don't include any prefix (e.g. 'Sentiment: ').\n",
      "\n",
      "---\n",
      "\n",
      "Country Keyword: spain\n",
      "\n",
      "Country Role: Country Keyword: Spain Excerpt: The mention of Spain in the text is related to the country's expected economic growth impact due to the next-generation EU funds, with an estimated 1.5% increase in GDP growth in Spain specifically.\n",
      "\n",
      "Excerpt: e specific products, and we are in constant cooperation with the government on this. but in terms of the impact, you were more asking for the impact, i guess. for the gdp, first of all, for the gdp, this next-generation eu, our bbva research team, they do expect a 1 percentage point impact in the gdp growth because of next-generation eu in 2022, and 1.5% percentage point increase in gdp growth in spain due to next-generation eu funds. so overall, economic situation will be helped from this. we have also deep dived into which sectors are going to be receiving this, which types of companies, what is the subsidy level, how much financing, additional financing those clients might be needing and so on. we do expect annually an 8% increase in the what we call new loan production in the commercia\n",
      "\n",
      "Reasoning: Let's think step by step in order to assess the expectation towards the solvency of Spain. The mention of Spain's expected economic growth impact due to the next-generation EU funds signifies a positive outlook for the country's financial stability. An estimated 1.5% increase in GDP growth specifically for Spain highlights potential economic growth and stability, which could improve the country's ability to meet its obligations.\n",
      "\n",
      "Answer: 2\n",
      "\n",
      "---\n",
      "\n",
      "Country Keyword: belgium\n",
      "\n",
      "Country Role: Country Keyword: Belgium Excerpt: The mention of Belgium in the text is related to the question about cash streaming and solvency in the country, particularly regarding the normalized cash upstream levels and how it may be impacted by Solvency II regulations and current interest rates.\n",
      "\n",
      "Excerpt: in, you take the worst case, but it could be, i don't know if it is the right word, a catastrophic event, in the sense very unlikely. ashik musaddi, jpmorgan - analyst okay. that's very clear, thank you. operator albert ploegh, ing. albert ploegh, ing financial markets - analyst two questions from my end. the first one, also a little bit related to the first question on the cash streaming and the belgium solvency. i think last year you guided that the normalized cash upstream was around a eur525 million kind of run rate. is that still a level you feel comfortable with for 2015, given solvency ii and given where rates are today? especially as, basically, the normalization should come mostly from the belgium unit, which last year streamed a little bit less capital, of course, to the holding.\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m assess the expectation towards the solvency of Belgium. The mention of Belgium in relation to cash streaming and solvency, particularly considering Solvency II regulations and current interest rates, raises concerns about the country's financial stability. The uncertainty surrounding normalized cash upstream levels and potential impacts from regulations and interest rates may indicate a somewhat negative outlook for Belgium's ability to meet its obligations.\n",
      "\n",
      "Answer: -1\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---CONTEXT---\n",
      "    You are an experienced financial analyst known for your expertise in evaluating and interpreting expectations related to the financial stability and solvency of countries.\n",
      "\n",
      "    ---TASK---\n",
      "    Please assess the expectation towards the solvency of the country mentioned in the given text excerpt. This includes evaluating the country's financial stability and ability to meet its obligations.    \n",
      "    \n",
      "    ---GUIDELINES---\n",
      "    - Use the following scale for your assessment: -2 = very negative, -1 = somewhat negative, 0 = neutral, 1 = somewhat positive, 2 = very positive\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Country Keyword: keyword that represents a country\n",
      "\n",
      "Country Role: role of the country in the excerpt of a financial services company's earnings call transcript\n",
      "\n",
      "Excerpt: excerpt from a financial services company's earnings conference call\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: one of: [-2, -1, 0, 1, 2]. Only respond wiht a single digit and nothing else. Don't include any prefix (e.g. 'Sentiment: ').\n",
      "\n",
      "---\n",
      "\n",
      "Country Keyword: spain\n",
      "\n",
      "Country Role: Country Keyword: Spain Excerpt: The mention of Spain in the text is related to the country's expected economic growth impact due to the next-generation EU funds, with an estimated 1.5% increase in GDP growth in Spain specifically.\n",
      "\n",
      "Excerpt: e specific products, and we are in constant cooperation with the government on this. but in terms of the impact, you were more asking for the impact, i guess. for the gdp, first of all, for the gdp, this next-generation eu, our bbva research team, they do expect a 1 percentage point impact in the gdp growth because of next-generation eu in 2022, and 1.5% percentage point increase in gdp growth in spain due to next-generation eu funds. so overall, economic situation will be helped from this. we have also deep dived into which sectors are going to be receiving this, which types of companies, what is the subsidy level, how much financing, additional financing those clients might be needing and so on. we do expect annually an 8% increase in the what we call new loan production in the commercia\n",
      "\n",
      "Reasoning: Let's think step by step in order to assess the expectation towards the solvency of Spain. The mention of Spain's expected economic growth impact due to the next-generation EU funds signifies a positive outlook for the country's financial stability. An estimated 1.5% increase in GDP growth specifically for Spain highlights potential economic growth and stability, which could improve the country's ability to meet its obligations.\n",
      "\n",
      "Answer: 2\n",
      "\n",
      "---\n",
      "\n",
      "Country Keyword: france\n",
      "\n",
      "Country Role: Country Keyword: France Excerpt: France is mentioned in relation to rate increases in the country, which is supporting the company's underwriting results.\n",
      "\n",
      "Excerpt: tter up than i expected, because i thought that the unipol portfolio will have a more worse loss ratio than our portfolio. but in the end, it played out that it is a little bit worse than the allianz average portfolio. therefore, the average combined loss ratio in italy, and that is a 20% portfolio expansion in italy, is actually looking very strong. that means supporting our underwriting result. france and germany continue to have rate increases, so also support from there. agcs and [euler are] holding up, and the turnaround candidates, i think, are on the right track. russia will just be diminished to almost nothing; brazil is on a good way, going forwards; the personal lines business, fireman's fund, is sold. and the commercial business, before we improve the loss ratios there, that is\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m assess the expectation towards the solvency of France. The mention of rate increases in France supporting the company's underwriting results indicates a positive financial outlook for the country. This suggests that the insurance market in France is performing well, which could contribute to the country's financial stability and ability to meet its obligations.\n",
      "\n",
      "Answer: 1\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt35turbo.inspect_history(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Evaluation\n",
    "\n",
    "Here, I just manually inspect what happens to control the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'valset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m custom_evaluation_function(validation_set\u001b[38;5;241m=\u001b[39m\u001b[43mvalset\u001b[49m, \n\u001b[1;32m      2\u001b[0m                            llm\u001b[38;5;241m=\u001b[39m full_llm_chain, \n\u001b[1;32m      3\u001b[0m                            metric_for_evaluation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluate_expectations\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      4\u001b[0m                            show_examples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'valset' is not defined"
     ]
    }
   ],
   "source": [
    "custom_evaluation_function(validation_set=valset, \n",
    "                           llm= full_llm_chain, \n",
    "                           metric_for_evaluation=\"evaluate_expectations\", \n",
    "                           show_examples=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt35turbo.inspect_history(n = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate = Evaluate(devset=valset, metric= evaluate_expectations_metric, num_threads=4, display_progress=True, display_table=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment_4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
